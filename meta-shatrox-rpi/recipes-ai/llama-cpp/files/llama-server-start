#!/bin/bash
################################################################################
# llama-server-start - Start llama.cpp HTTP API server
################################################################################

MODEL_DIR="/usr/share/llama-models"
DEFAULT_MODEL="$MODEL_DIR/default"

# Check if model exists
if [ ! -f "$DEFAULT_MODEL" ]; then
    echo "Error: Default model not found at $DEFAULT_MODEL"
    echo "Available models:"
    ls -lh "$MODEL_DIR"/*.gguf 2>/dev/null || echo "No models found"
    exit 1
fi

HOST="${1:-0.0.0.0}"
PORT="${2:-8080}"

echo "Starting llama.cpp server..."
echo "Model: $DEFAULT_MODEL"
echo "Listening on: http://$HOST:$PORT"
echo "API endpoint: http://$HOST:$PORT/v1/chat/completions"
echo "Health check: http://$HOST:$PORT/health"
echo "----------------------------------------"

# Start server with settings optimized for 4GB RAM
exec /usr/bin/llama-server \
    -m "$DEFAULT_MODEL" \
    --host "$HOST" \
    --port "$PORT" \
    --ctx-size 2048 \
    --threads 4 \
    --no-mmap \
    --log-disable

exit $?
