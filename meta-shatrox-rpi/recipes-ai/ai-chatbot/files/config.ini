[ollama]
# Ollama server configuration
# Use 'local' for local Ollama server, or 'IP:PORT' for network server (e.g., 192.168.2.170:11434)
ollama_host = local
# Vision model to use on network server (if ollama_host is not 'local')
network_vision_model = moondream
# Text model to use on network server (if ollama_host is not 'local')
network_text_model = llama3.2:3b
# Connection timeout for network Ollama (seconds)
network_timeout = 5

[llm]
# System prompt for concise answers (robot personality)
system_prompt = You are a helpful robot. Answer in 1 sentence maximum. Be direct and concise.
# Ollama models
text_model = llama3.2:1b
vision_model = moondream
max_tokens = 50
temperature = 0.7

[vosk]
# VOSK ASR settings
model_path = /usr/share/vosk-models/default
sample_rate = 16000

[audio]
# Audio device will be auto-detected by detect-audio.sh
# USB Audio Device is typically card 0 on RPi OS
microphone_device = plughw:0,0
speaker_device = auto
sample_rate = 16000

[camera]
enable = true
resolution = 640x480

[behavior]
# Auto-reset conversation after 5 minutes of inactivity
chat_history_timeout = 300
max_history_messages = 10

[wake_word]
# Enable wake word detection (set to true to enable always-listening mode)
enabled = true
# Wake word model path (hey_jarvis_v0.1 is temporary, will be replaced with hey_ruby_v1)
model_path = /usr/share/openwakeword-models/hey_jarvis_v0.1.onnx
# Detection threshold (0.0-1.0, higher = less sensitive)
threshold = 0.5
# Feedback sound on detection
feedback_sound = /usr/share/sounds/wake.wav
# Command timeout after wake word (seconds)
command_timeout = 5
